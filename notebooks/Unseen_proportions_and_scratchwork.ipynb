{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/alex/Data/Backup_root/Backup_docs/git/reddit-sarc/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rnn_util import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12704751\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open(FULL_COMMENTS, 'r') as f:\n",
    "    comments = json.load(f)\n",
    "print(len(comments))\n",
    "print(type(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users seen in train set:  168304\n",
      "Num unseen val 7535 out of 12854; proportion is 0.5861988486074374\n",
      "Num unseen holdout 7637 out of 12856; proportion is 0.5940416925948974\n",
      "Num unseen test 43544 out of 78425; proportion is 0.5552311125278929\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from random import shuffle\n",
    "from copy import copy\n",
    "\n",
    "comment_sets = []\n",
    "with open(FULL_TRAIN_BALANCED, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "        #ancestors_idx = row[0].split(' ')\n",
    "        responses_idx = row[1].split(' ')\n",
    "        comment_sets.append(responses_idx)\n",
    "        #labels = np.array(row[2].split(' '), dtype=np.int32)\n",
    "\n",
    "#keys = [k for k in comments.keys()]\n",
    "#shuffle(keys)\n",
    "\n",
    "shuffle(comment_sets)\n",
    "num_sets = len(comment_sets)\n",
    "\n",
    "train_idx = int(num_sets*.9)\n",
    "val_idx = int(num_sets*.95)\n",
    "train_sets, val_sets, holdout_sets = comment_sets[:train_idx], comment_sets[train_idx : val_idx], comment_sets[val_idx :]\n",
    "\n",
    "seen_users = set()\n",
    "for s in train_sets:\n",
    "    for k in s:\n",
    "        v = comments[k]\n",
    "        seen_users.add(v['author'])\n",
    "    \n",
    "print('Num users seen in train set: ', len(seen_users))\n",
    "    \n",
    "num_unseen_val = 0\n",
    "num_val = 0\n",
    "for s in val_sets:\n",
    "    for k in s:\n",
    "        v = comments[k]\n",
    "        if not v['author'] in seen_users: num_unseen_val += 1\n",
    "        num_val += 1\n",
    "\n",
    "print('Num unseen val {} out of {}; proportion is {}'.format(num_unseen_val, num_val, num_unseen_val / num_val))\n",
    "\n",
    "num_unseen_holdout = 0\n",
    "num_holdout = 0\n",
    "for s in holdout_sets:\n",
    "    for k in s:\n",
    "        v = comments[k]\n",
    "        if not v['author'] in seen_users: num_unseen_holdout += 1\n",
    "        num_holdout += 1\n",
    "    \n",
    "print('Num unseen holdout {} out of {}; proportion is {}'.format(num_unseen_holdout, num_holdout, num_unseen_holdout / num_holdout))\n",
    "\n",
    "num_unseen_test = 0\n",
    "num_test = 0\n",
    "with open(POL_TEST_UNBALANCED, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "        responses_idx = row[1].split(' ')\n",
    "        for k in responses_idx:\n",
    "            v = comments[k]\n",
    "            if not v['author'] in seen_users: num_unseen_test += 1\n",
    "            num_test += 1\n",
    "print('Num unseen test {} out of {}; proportion is {}'.format(num_unseen_test, num_test, num_unseen_test / num_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in politics balanced configs:\n",
      "hidden_dim\n",
      "dropout\n",
      "l2_lambda\n",
      "second_linear_layer\n",
      "\n",
      "\n",
      "Differences in full balanced configs:\n",
      "hidden_dim\n",
      "dropout\n",
      "l2_lambda\n",
      "second_linear_layer\n"
     ]
    }
   ],
   "source": [
    "from test_configs import *\n",
    "\n",
    "print(\"Differences in politics balanced configs:\")\n",
    "for k in B2.keys():\n",
    "    if not (B2[k] == B3[k] == B4[k]):\n",
    "        print(k)\n",
    "        \n",
    "\n",
    "print(\"\\n\\nDifferences in full balanced configs:\")\n",
    "for k in C2.keys():\n",
    "    if not (C2[k] == C3[k] == C4[k]):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/alex/Data/Backup_root/Backup_docs/git/reddit-sarc/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src\n",
    "from rnn_util import *\n",
    "from test_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasttext_lookup, fasttext_word_to_idx = load_embeddings_by_index(FASTTEXT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = build_and_split_dataset(word_to_idx=fasttext_word_to_idx, **B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 holdout\n"
     ]
    }
   ],
   "source": [
    "first_holdout = list(dataset['raw_holdouts'].keys())[0]\n",
    "print(first_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ancestors': ['Here\\'s a lovely story: A pair of rich assholes move into one of the most popular ballooning spots in the country, build a gigantic \"Moorish fortress castle\", &amp; then start suing the balloonists out of existence because they don\\'t like people flying over their exclusive getaway property.'],\n",
       " 'responses': ['See, the rich ARE job creators.',\n",
       "  \"By that logic couldn't I sue the US Navy for all the planes that fly over my house on the way to and from the Naval air station near by?\"],\n",
       " 'labels': array([1, 0], dtype=int32),\n",
       " 'ancestor_authors': ['anutensil'],\n",
       " 'response_authors': ['fingers', 'njm1314'],\n",
       " 'ancestor_subreddits': ['politics'],\n",
       " 'response_subreddits': ['politics', 'politics']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['raw_holdouts'][first_holdout][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_example = dataset['holdout_datas'][first_holdout]['X'][0]\n",
    "second_example = dataset['holdout_datas'][first_holdout]['X'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasttext_idx_to_word = {idx:word for word,idx in fasttext_word_to_idx.items()}\n",
    "fasttext_idx_to_word[0] = 'UNK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(ex):\n",
    "    ex = [int(x) for x in ex]\n",
    "    num_trailing_zeroes = 0\n",
    "    for i in range(1, len(ex)):\n",
    "        if ex[-i] == 0: num_trailing_zeroes = i\n",
    "        else: break\n",
    "    ex = ex[:len(ex) - num_trailing_zeroes]\n",
    "    return ' '.join([fasttext_idx_to_word[x] for x in ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By that logic couldn 't I sue the US Navy for all the planes that fly over my house on the way to and from the Naval air station near by ?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(second_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_default",
   "language": "python",
   "name": "conda_default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
